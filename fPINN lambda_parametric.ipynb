{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подключение библиотек ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from scipy.special import gamma, erfc\n",
    "from scipy.integrate import quad\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pyfod.fod import caputo\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка СUDA ##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция Миттаг-Леффлера ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLF(z, alpha, N=10000):\n",
    "    # tic = time.perf_counter()\n",
    "\n",
    "    # Преобразование типов данных\n",
    "    if not isinstance(z, np.ndarray):\n",
    "        if isinstance(z, torch.Tensor):\n",
    "            z = z.numpy()\n",
    "        else:\n",
    "            z = np.array(z)\n",
    "    z = np.atleast_1d(z)\n",
    "    \n",
    "    # Случаи для различных параметров alpha\n",
    "    if alpha == 0:\n",
    "        return 1/(1 - z)\n",
    "    elif alpha == 1:\n",
    "        return np.exp(z)\n",
    "    elif alpha == 1/2:\n",
    "        return np.exp(z ** 2) * erfc(-z)\n",
    "    elif alpha > 1 or all(z > 0):\n",
    "        k = np.arange(N)\n",
    "        return np.polynomial.polynomial.polyval(z, 1 / gamma(alpha * k + 1))\n",
    "        # j = np.arange(N).reshape(-1, 1)\n",
    "        # E = (np.power(z, j)) / gamma(alpha * j + 1)\n",
    "        # return torch.tensor(np.sum(E, axis=0))\n",
    "\n",
    "    def _MLF(z, alpha):\n",
    "        if z < 0:\n",
    "            # def f(x): return ((np.exp(-x * (-z)**(1 / alpha)) * x**(alpha - 1) * np.sin(np.pi * alpha))\n",
    "            #                   / (x**(2 * alpha) + 2 * x**alpha * np.cos(np.pi * alpha) + 1))\n",
    "            def f(x): return ((np.exp(-x * np.power(-z, 1 / alpha)) * np.power(x, alpha - 1) * np.sin(np.pi * alpha))\n",
    "                    / (np.power(x, 2 * alpha) + 2 * np.power(x, alpha) * np.cos(np.pi * alpha) + 1))\n",
    "            return 1 / np.pi * quad(f, 0, np.inf)[0]\n",
    "        elif z == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return MLF(z, alpha)\n",
    "\n",
    "    # toc = time.perf_counter()\n",
    "    # print(f\"Time = {toc-tic}\")\n",
    "    return np.vectorize(_MLF)(z, alpha)\n",
    "\n",
    "\n",
    "res = MLF(1, 1)\n",
    "print(f\"MLF = {res}, type={type(res)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "x = np.linspace(-10, 1, 100)\n",
    "plt.grid()\n",
    "for i in range(1, 11):\n",
    "    plt.plot(x, MLF(x, i/10), label=\"alpha = \"+str(i/10))\n",
    "plt.legend()\n",
    "plt.ylim(-2, 5)\n",
    "plt.xlim(x[0], x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "x = np.linspace(-10, 1, 100)\n",
    "plt.grid()\n",
    "plt.plot(x, MLF(x, 0.1), label=\"alpha = 0.1\")\n",
    "plt.legend()\n",
    "plt.ylim(-2, 5)\n",
    "plt.xlim(x[0], x[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель нейронной сети ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_hidden, size_hidden, activation=nn.Tanh()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_function = activation\n",
    "\n",
    "        self.layer_input = nn.Linear(2, size_hidden).double()\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(size_hidden, size_hidden).double() for _ in range(num_hidden - 1)])\n",
    "        self.layer_output = nn.Linear(size_hidden, 1).double()\n",
    "    \n",
    "    def forward(self, x, p):\n",
    "        x = x.reshape(-1, 1)\n",
    "        x = x.type(torch.double)\n",
    "        p = p.type(torch.double)\n",
    "        if p.shape == torch.Size([1]):\n",
    "            x  = x.reshape(-1,)\n",
    "            x_p = torch.cat([x, p])\n",
    "        else: \n",
    "            x_p = torch.cat([x, p], dim=1)\n",
    "        x_p = self.layer_input(x_p)\n",
    "        x_p = self.activation_function(x_p)\n",
    "        for layer in self.hidden_layers:\n",
    "            x_p = layer(x_p)\n",
    "            x_p = self.activation_function(x_p)\n",
    "        output = self.layer_output(x_p)\n",
    "        output = output.type(torch.float64)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация весов ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Текущий результат работы сети ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_fn(nn, x, p):\n",
    "    return nn(x, p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление производной ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(nn, alpha, x, p):\n",
    "    def f(x, p=None):\n",
    "        x = torch.from_numpy(x)\n",
    "        tmp = torch.full((len(x), 1), p.item())\n",
    "        return net_fn(nn, x, tmp)\n",
    "    result = torch.zeros((len(x), 1))\n",
    "    x = torch.unique(x)\n",
    "    p = torch.unique(p)\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(p)):\n",
    "            foo = partial(f, p=p[j])\n",
    "            fd = caputo(f=foo, alpha=alpha, lower=0, upper=x[i], quadrature='rs', n=1000)\n",
    "            result[i * len(p) + j] = fd['fd']\n",
    "    return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция ошибки ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(nn, alpha, lin, x=None, p=None, additional_condition=None, verbose=False):\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    interior_loss = 0.\n",
    "\n",
    "    if lin == \"Linear\":\n",
    "        lmbd = additional_condition\n",
    "        # Начальное условие\n",
    "        x_init = torch.zeros_like(p)\n",
    "        initial_condition = torch.ones_like(x_init)\n",
    "        \n",
    "        initial_loss = net_fn(nn, x_init, p) - initial_condition\n",
    "        initial_loss = mse_loss(initial_loss, torch.zeros_like(initial_loss))\n",
    "        \n",
    "        # Внутренняя ошибка\n",
    "        grid = torch.meshgrid(x[1:].reshape(-1,), p.reshape(-1,), indexing='ij')\n",
    "        x_grid = grid[0].flatten().reshape(-1, 1)\n",
    "        p_grid = grid[1].flatten().reshape(-1, 1)\n",
    "        # lmbd_tmp = torch.full((len(x_grid), 1), lmbd)\n",
    "        \n",
    "        interior_loss = df(nn, alpha, x_grid, p_grid) - p_grid * net_fn(nn, x_grid, p_grid)\n",
    "        interior_loss = mse_loss(interior_loss, torch.zeros_like(interior_loss)) \n",
    "        \n",
    "    elif lin == \"Nonlinear\":\n",
    "        m = additional_condition\n",
    "        # Начальное условие\n",
    "        x_init = torch.zeros_like(p)\n",
    "        \n",
    "        initial_loss = net_fn(nn, x_init, p) - x_init\n",
    "        initial_loss = mse_loss(initial_loss, torch.zeros_like(initial_loss))\n",
    "        \n",
    "        # Внутренняя ошибка\n",
    "        grid = torch.meshgrid(x[1:].reshape(-1,), p.reshape(-1,), indexing='ij')\n",
    "        x_grid = grid[0].flatten().reshape(-1, 1)\n",
    "        p_grid = grid[1].flatten().reshape(-1, 1)\n",
    "        \n",
    "        interior_loss = df(nn, alpha, x_grid, p_grid) - p_grid * (torch.abs(net_fn(nn, x_grid, p_grid)) ** m)\n",
    "        interior_loss = mse_loss(interior_loss, torch.zeros_like(interior_loss)) \n",
    "        \n",
    "    # print(f\"Initial loss: {initial_loss}\")\n",
    "    # print(f\"Interior loss: {interior_loss}\")\n",
    "\n",
    "    loss = initial_loss + interior_loss     \n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл обучения ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(nn, alpha, loss_fn, optimizer, scheduler=None, n_epochs=1000):\n",
    "    train_loss = torch.zeros(n_epochs)\n",
    "    \n",
    "    def closure():\n",
    "        if torch.is_grad_enabled():\n",
    "            optimizer.zero_grad()\n",
    "        loss = loss_fn(nn, alpha)\n",
    "        if loss.requires_grad:\n",
    "            loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "                       \n",
    "        if isinstance(optimizer, torch.optim.LBFGS):\n",
    "            optimizer.step(closure)\n",
    "            loss = closure()\n",
    "        else:\n",
    "            loss = loss_fn(nn, alpha)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, learning rate = {optimizer.param_groups[-1]['lr']}, loss={loss},\\n\")\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            break\n",
    "        \n",
    "    return nn, train_loss      \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Параметры задач ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Точные решения ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_lin(x, alpha, y0_lin, lambda_lin):\n",
    "    solution = y0_lin * ml(lambda_lin * np.power(x, alpha), alpha)\n",
    "    return solution\n",
    "\n",
    "def solution_nonlin(x, alpha, m_nonlin, lambda_nonlin):\n",
    "    gm = alpha / (1 - m_nonlin)\n",
    "    tmp = gamma(gm + 1) / (lambda_nonlin * gamma(gm - alpha + 1))\n",
    "    B = np.power(tmp, 1 / (m_nonlin - 1))\n",
    "    solution = B * np.power(x, gm)\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Equations = {\"Linear\" : solution_lin, \"Nonlinear\" : solution_nonlin}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.linspace(0, 1, 1000)\n",
    "# plt.grid()\n",
    "# plt.plot(x, Equations['Linear'](x, alpha=0.1, y0_lin=0.5, lambda_lin=0.5))\n",
    "# # plt.plot(x, Equations['Nonlinear'](x, alpha=0.1, m_nonlin=0.5, lambda_nonlin=1))\n",
    "# plt.legend(Equations.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение сети ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры сети ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = 'Nonlinear'\n",
    "cond = 0.5\n",
    "alpha = 0.5\n",
    "\n",
    "num_hidden = 4\n",
    "size_hidden = 10\n",
    "activation_function = nn.Tanh()\n",
    "\n",
    "NN = Net(num_hidden, size_hidden, activation_function)\n",
    "NN.apply(init_weights)\n",
    "\n",
    "learning_rate_Adam = 0.01\n",
    "learning_rate_LBFGS = 0.01\n",
    "epochs_Adam = 15000\n",
    "epochs_LBFGS = 1000\n",
    "\n",
    "optimizer_adam = torch.optim.AdamW(NN.parameters(), lr=learning_rate_Adam, weight_decay=0.00001) # , weight_decay=0.00001\n",
    "optimizer_lbfgs = torch.optim.LBFGS(NN.parameters(), lr=learning_rate_LBFGS)\n",
    "scheduler = ReduceLROnPlateau(optimizer_adam, 'min', factor=0.1, patience=2000, min_lr=1e-6)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка на заданном интервале ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 1, 10).reshape(-1, 1)\n",
    "p = torch.linspace(0.1, 1, 10).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = partial(loss_function, lin=problem, x=x, p=p, additional_condition=cond, verbose=True)\n",
    "PINN_Adam, train_loss_Adam = training_loop(\n",
    "    nn=NN,\n",
    "    alpha=alpha,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer_adam,\n",
    "    scheduler=scheduler,\n",
    "    n_epochs=epochs_Adam\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = partial(loss_function, lin=problem, x=x, p=p, additional_condition=cond, verbose=True)\n",
    "# PINN_LBFGS, train_loss_LBFGS = training_loop(\n",
    "#     nn=NN,\n",
    "#     alpha=alpha,\n",
    "#     loss_fn=loss_fn,\n",
    "#     optimizer=optimizer_lbfgs,\n",
    "#     n_epochs=epochs_LBFGS\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN = PINN_Adam\n",
    "x_eval = torch.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "if problem == 'Linear':\n",
    "    lmbd = 0.9\n",
    "    cond_eval = torch.full(x_eval.shape, lmbd)\n",
    "    nn_approx = PINN(x_eval, cond_eval).detach().numpy()\n",
    "    sol = Equations[problem](x_eval, alpha, y0_lin=1, lambda_lin=lmbd)\n",
    "elif problem == 'Nonlinear':\n",
    "    m = 0.6\n",
    "    cond_eval = torch.full(x_eval.shape, m)\n",
    "    nn_approx = PINN(x_eval, cond_eval).detach().numpy()\n",
    "    sol = Equations[problem](x_eval, alpha, m_nonlin=m, lambda_nonlin=cond)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества модели ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(solution, approximation):\n",
    "    if isinstance(solution, torch.Tensor):\n",
    "        solution = solution.detach().numpy()\n",
    "    if np.any(np.isnan(approximation)):\n",
    "        return np.nan, np.nan, np.nan\n",
    "    MAE = mean_absolute_error(solution[1:], approximation[1:])\n",
    "    MRE = np.mean(np.fabs(approximation[1:] - solution[1:]) / np.fabs(solution[1:])) * 100\n",
    "    InitLoss = float(np.fabs(solution[0] - approximation[0]))\n",
    "    # print(f\"Mean absolute error (MAE) = {MAE:.5f}\\nMean relative error (MRE) = {MRE:.5f}%\\nLoss on initial condition = {InitLoss:.5f}\")\n",
    "    return MAE, MRE, InitLoss\n",
    "\n",
    "# MAE, MRE, InitLoss = error(sol, nn_approx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отрисовка графиков ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(x, solution, approximation, name=None):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    \n",
    "    plt.plot(x, approximation, '-.', label='PINN')\n",
    "    plt.plot(x, solution, label='Solution')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "    MAE, MRE, InitLoss = error(solution, approximation)\n",
    "    plt.title(f\"Mean absolute error (MAE) = {MAE:.5}\\nLoss on initial condition = {InitLoss:.5}\")\n",
    "    \n",
    "    if name is not None:\n",
    "        plt.savefig(f\"{name}.png\")\n",
    "    \n",
    "    # plt.cla()\n",
    "    return MAE, MRE, InitLoss\n",
    "    \n",
    "plot_graph(x_eval, sol, nn_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN = PINN_Adam\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(-0.1, 1.1)\n",
    "x_eval = torch.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "for l in [0.1, 0.2, 0.5, 0.8, 0.9, 1]:\n",
    "    lmbd = l\n",
    "    cond_eval = torch.full(x_eval.shape, lmbd)\n",
    "    nn_approx = PINN(x_eval, cond_eval).detach().numpy()\n",
    "    sol = Equations[problem](x_eval, alpha, y0_lin=1, lambda_lin=lmbd)\n",
    "    plt.plot(x_eval, nn_approx, '-.', label=f'PINN λ={lmbd}')\n",
    "    plt.plot(x_eval, sol, label=f'Solution λ={lmbd}')\n",
    "    plt.grid()\n",
    "    plt.legend(ncols=2) # ncols=2, loc='upper left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
